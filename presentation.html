<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Scalable Maximum Likelihood Estimation of Multilevel Latent Variable Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Øystein Sørensen" />
    <script src="libs/header-attrs-2.17/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Scalable Maximum Likelihood Estimation of Multilevel Latent Variable Models
]
.subtitle[
## OCBE Lunch Seminar
]
.author[
### Øystein Sørensen
]
.institute[
### Department of Psychology, University of Oslo
]

---








# Outline

- Overview of latent variable models

- A framework for semiparametric latent variable modeling

- Maximum likelihood estimation

- Applications


---

class: inverse, middle, center

# Overview of Latent Variable Models


---

## Confirmatory Factor Analysis

A set of items measure some underlying construct(s)

`$$\mathbf{y}_{j} = \boldsymbol{\Lambda} \boldsymbol{\eta}_{j} + \boldsymbol{\epsilon}_{j}, \qquad \boldsymbol{\eta}_{j} \sim N\left(\mathbf{0}, \boldsymbol{\Psi}\right)$$`

&lt;br&gt;
&lt;center&gt;
&lt;img src="figures/cfa.png" width=400&gt;
&lt;/center&gt;

---

## Item Response Theory

- Kind of factor analysis with binomial measurements `\(\mathbf{y}_{j} \sim \text{Binom}\left(\boldsymbol{\mu}_{j}\right)\)`.

- Predictor for item `\(i\)`, subject `\(j\)`

`$$\nu_{ij} = a_{i} (\eta_{j} - b_{i})$$`



&lt;center&gt;
&lt;img src="figures/irt.png" width=400&gt;
&lt;/center&gt;

---


## Structural Equation Models

&lt;center&gt;
&lt;img src="figures/sem.png" width=450&gt;
&lt;/center&gt;


---


## Structural Equation Models




- Measurement model for response:

`$$\mathbf{y}_{j} = \boldsymbol{\Lambda}_{y} \boldsymbol{\eta}_{j} + \boldsymbol{\epsilon}_{j}, \qquad \boldsymbol{\epsilon}_{j} \sim N(\mathbf{0}, \boldsymbol{\Psi}_{y})$$`

- Measurement model for predictors:

`$$\mathbf{x}_{j} = \boldsymbol{\Lambda}_{x} \boldsymbol{\xi}_{j} + \boldsymbol{\delta}_{j}, \qquad \boldsymbol{\xi}_{j} \sim N\left(\mathbf{0}, \boldsymbol{\Psi}_{x}\right)$$`

- Structural model:

`$$\boldsymbol{\eta}_{j} = \mathbf{B}\boldsymbol{\eta}_{j} + \boldsymbol{\Gamma} \boldsymbol{\xi}_{j} + \boldsymbol{\zeta}_{j}, \qquad \boldsymbol{\zeta}_{j} \sim N\left(\mathbf{0}, \boldsymbol{\Psi}\right)$$`

---


## Generalized Linear Mixed Models

- Response `\(\mathbf{y}_{j}\)` from exponential family with mean `\(\boldsymbol{\mu}_{j}\)`

- `\(\boldsymbol{\mu}_{j}\)` depends on latent variables `\(\boldsymbol{\eta}_{j}\)`

`$$\boldsymbol{\mu}_{j} = g^{-1}\left(\mathbf{X}_{j} \boldsymbol{\beta} + \mathbf{Z}_{j} \boldsymbol{\eta}_{j}\right),  \qquad \boldsymbol{\eta}_{j} \sim N\left(\mathbf{0}, \boldsymbol{\Psi}\right)$$`


---

class: inverse, middle, center

# Generalized Linear Latent and Mixed Models

All models mentioned above, and many more, are special cases of GLLAMM.

.footnote[Rabe-Hesketh, S., Skrondal, A., &amp; Pickles, A. (2004). Generalized multilevel structural equation modeling. Psychometrika, 69(2), 167--190. https://doi.org/10.1007/BF02295939]

---


## GLLAMM


- Exponential family response, possibly of mixed type.

- `\(L\)` grouping levels. `\(M_{l}\)` latent variables at `\(l\)`th level.

Linear predictor for observational unit `\(i\)`

`$$\nu_{i} = \mathbf{x}_{i}^{T}\boldsymbol{\beta} + \sum_{l=2}^{L} \sum_{m=1}^{M_{l}} \eta_{m}^{(l)} \mathbf{z}_{mi}^{(l)}{}^{T} \boldsymbol{\lambda}_{m}^{(l)}$$`

Structural model for cluster `\(j\)`

`$$\boldsymbol{\eta}_{j} = \mathbf{B}\boldsymbol{\eta}_{j} +\boldsymbol{\Gamma} \mathbf{w}_{j} + \boldsymbol{\zeta}_{j}, \qquad \boldsymbol{\zeta}_{j} \sim N\left(\mathbf{0}, \boldsymbol{\Psi}\right)$$`



---

## GLLAMM

Lots of applications, but some limitations:

1. Strictly parametric.

2. New results on mixed model estimation can be used to speed up the algorithms, particularly for crossed random effects.

3. Stata based. Limited availability in R.

We'll address these things one at a time.


---

class: inverse, middle, center

# Thing 1

"GLLAMMs are strictly parametric."


---

## Semiparametric Estimation often Needed

- The world is nonlinear.

- Easier and safer to learn the nonlinear form from the data, rather than pulling a parameteric model out of the hat.

Example: how many words people remember, plotted against age.

&lt;center&gt;
&lt;img src="figures/dspan_plot.png" width=500&gt;
&lt;/center&gt;


---

## Generalized Additive Latent and Mixed Models 

Extension of GLLAMM framework to incorporate smooth functions of observed variables.

- Linear predictor

`$$\nu_{i} = \color{red}{\sum_{s=1}^{S}f_{s}\left(\mathbf{x}_{i}\right)} + \sum_{l=2}^{L} \sum_{m=1}^{M_{l}} \eta_{m}^{(l)} \mathbf{z}_{mi}^{(l)}{}^{T} \boldsymbol{\lambda}_{m}^{(l)}$$`

- Structural model

`$$\boldsymbol{\eta}_{j} = \mathbf{B}\boldsymbol{\eta}_{j} + \color{red}{\mathbf{h}\left( \mathbf{w}_{j} \right)} + \boldsymbol{\zeta}_{j}, \qquad \boldsymbol{\zeta}_{j} \sim N\left(\mathbf{0}, \boldsymbol{\Psi}\right)$$`

- `\(f_{s}(\cdot)\)` and `\(\mathbf{h}(\cdot)\)` composed of regression splines. Second derivative penalization.


---

## GLLAMMs with Semiparametric Functions

- Semiparametric model can be represented by a standard GLLAMM.

  - Part of basis in penalty nullspace becomes fixed effects.

  - Part of basis in penalty range space becomes random effects.

  - Inverse smoothing parameters become new variance components.

--

- Gets a bit nasty. In the words of reviewer 3

&gt; ... the notation in this paper is so dreadful that I considered declining to review it. If the notation is not thoroughly cleaned up in a revision, I will not review it again. I only begin to touch the surface in these comments.
(...) The lack of consistency seems almost sadistic, as if it was done to purposively make even simple expressions inscrutable! 




---

class: inverse, middle, center

# Anyhow




---

class: inverse, middle, center

# Thing 2

"New results on mixed model estimation can be used to speed up the algorithms, particularly for crossed random effects."


---

class: inverse, middle, center

# A Scalable Algorithm

Laplace approximation, sparse matrices, and autodiff to the rescue

---

# Likelihood

- Transform latent variables through `\(\boldsymbol{\Lambda} \mathbf{u} = \boldsymbol{\zeta}\)`, where `\(\boldsymbol{\Lambda}^{T}\boldsymbol{\Lambda} = \boldsymbol{\Psi}\)`, so `\(\mathbf{u} \sim N(\mathbf{0}, \phi_{1} \mathbf{I})\)`. `\(\phi_{1}\)` is a reference level, since the dispersion may differ between observations.


--


- The marginal likelihood, integrating over `\(\mathbf{u} \in \mathbb{R}^{r}\)` is

`$$L\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \boldsymbol{\phi}\right)  = \\ \left(2 \pi \phi_{1}\right)^{-r/2}  \int_{\mathbb{R}^{r}} \exp\left( g\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \boldsymbol{\phi}, \mathbf{u}\right) \right) \text{d} \mathbf{u}$$`

where

`$$g\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \boldsymbol{\phi}, \mathbf{u}\right) = \\ 
\mathbf{y}^{T} \mathbf{W}\boldsymbol{\nu} - d\left(\boldsymbol{\nu}\right)^{T} \mathbf{W}\mathbf{1}_{n}  + c\left(\mathbf{y}, \boldsymbol{\phi}\right)^{T} \mathbf{1}_{n} - \left(2\phi_{1}\right)^{-1} \left\| \mathbf{u} \right\|^{2}$$`

- Just like a generalized linear mixed model, except that everything depends on the factor loadings and coefficients for regressions between latent variables. Doubly nonlinear in a sense.




---

## Laplace Approximation

- First goal, maximize the exponent in the integrand at fixed parameters.

`$$\tilde{\mathbf{u}} = \underset{\mathbf{u}}{\text{argmax}} \left\{ g\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \phi, \mathbf{u}\right) \right\}$$`

--

- Analytical expressions for gradient and Hessian

`$$\nabla g = \boldsymbol{\Lambda}^{T} \mathbf{Z}^{T} \mathbf{W}\left( \mathbf{y} - \boldsymbol{\mu} \right)  - \left(1/\phi_{1}\right)\mathbf{u}$$`

`$$\mathbf{H}_{g} =-  \boldsymbol{\Lambda}^{T} \mathbf{Z}^{T} \mathbf{V}  \mathbf{Z} \boldsymbol{\Lambda} - \left(1/\phi_{1}\right) \mathbf{I}_{r}$$`

- Full Newton method:

$$ \mathbf{H}_{g}^{(k)}\boldsymbol{\delta}^{(k)} =  \nabla g^{(k)}$$

and then updating `\(\mathbf{u}^{(k+1)} = \mathbf{u}^{(k)} + \gamma \boldsymbol{\delta}^{(k)}\)` for some adaptively chosen stepsize `\(\gamma\)`.



---

## Laplace Approximation

- Linear system

`$$\mathbf{H}_{g}^{(k)}\boldsymbol{\delta}^{(k)} = \nabla g^{(k)}$$`

- Hessian `\(\mathbf{H}_{g}^{(k)}\)` is typically **really really sparse**. Sparse Cholesky factorization

`$$\mathbf{L}^{(k)} \mathbf{D}^{(k)} \mathbf{L}^{(k)}{}^{T} = \mathbf{P} \mathbf{H}_{g}^{(k)}\mathbf{P}^{T}$$`

- Fill-reducing permutation `\(\mathbf{P}\)` only depends on location of structural zeros, so can be computed off-line.

--

- The system we solve, `\(Ax=b\)`, is

`$$\underbrace{\mathbf{L}^{(k)} \mathbf{D}^{(k)} \mathbf{L}^{(k)T} \mathbf{P}}_{A} \underbrace{\boldsymbol{\delta}^{(k)}}_{x} = \underbrace{\mathbf{P} \left( \boldsymbol{\Lambda}^{T} \mathbf{Z}^{T} \mathbf{W}^{(k)} \left( \mathbf{y} - \boldsymbol{\mu}^{(k)}\right)- (1/\phi_{1}^{(k)}) \mathbf{u}^{(k)}\right)}_{b}$$`


---

## Log-Likelihood

- At convergence, plug in conditional modes `\(\tilde{\mathbf{u}}\)` and update terms that depend on `\(\mathbf{u}\)`.

- Laplace approximate marginal log-likelihood

`$$\log L\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \boldsymbol{\phi}\right) = \\ \mathbf{y}^{T} \mathbf{W}\boldsymbol{\nu} - d\left(\boldsymbol{\nu}\right)^{T} \mathbf{W}\mathbf{1}_{n}  + c\left(\mathbf{y}, \boldsymbol{\phi}\right)^{T} \mathbf{1}_{n} - \left(2\phi_{1}\right)^{-1} \left\| \tilde{\mathbf{u} }\right\|^{2} - (1/2)\log \text{tr}\left(\mathbf{D}\right)$$`

---

# Maximizing the Marginal Likelihood

- Extend C++ library `Eigen` to allow sparse matrix computations with dual types from `autodiff`, rather than `double`/`float`.
  
- Exact gradients of log-likelihood wrt all parameters plugged into quasi-Newton L-BFGS-B algorithm to find maximum.

- At convergence, exact Hessian computed using second-order dual numbers.


.footnote[https://eigen.tuxfamily.org/, https://autodiff.github.io/]

---

class: inverse, middle, center

![](figures/newton.png)

Isaac and his cousin Fred Quasi-Newton inventing the rainbow.

---

class: inverse, middle, center


# Thing 3

Stata based. Limited availability in R.


---

## Implementation

- R package `galamm`, so far only at https://lcbc-uio.github.io/galamm/.

- Supports a wide range of models, including

  - Mixed response types
  
  - Crossed random effects
  
  - Semiparametric terms
  
  - Heteroscedasticity
  

- User-friendly API will come when smoke has cleared.


---

class: inverse, middle, center

# Application Example


---

# Effect of Socioeconomic Status on the Brain

- A Lancet report has suggested that low socioeconomic status is a risk factor for late life dementia, through its impact on brain structure.

- Lifespan brain development is pretty nonlinear. Example below shows volume of hippocampus. How does socioeconomic status interact with this curve?

&lt;center&gt;
&lt;img src="figures/mri.png" width=400&gt;
&lt;/center&gt;

---

# Effect of Socioeconomic Status on the Brain




- Latent socioeconomic status defined by income and education level. For children, parents' values were used.

- The model:

`$$y_{i} = \mathbf{d}_{\text{s},i}'\boldsymbol{\beta}_{\text{s}} + d_{h,i}\left\{ \mathbf{x}_{\text{h},i}' \boldsymbol{\beta}_{\text{h}} + f\left(a_{i}\right)\right\} +  \eta_{1} \mathbf{z}_{i}' \boldsymbol{\lambda} + d_{\text{h},i} \eta_{2} + \epsilon_{i}$$`

- Terms:

  - `\(\mathbf{d}_{s,i}\)`, dummy for socioeconomic status items.
  - `\(d_{h,i}\)`: dummy for hippocampus volume.
  - `\(f(a_{i})\)` effect of age on hippocampus volume.
  - `\(\eta_{1}\)`: latent socioeconomic status.
  - `\(\eta_{2}\)`: random intercept for hippocampus volume.
  - `\(\epsilon_{i}\)`: residual varying between groups of variables (income, education, hippocampus).
  

---

# Effect of Socioeconomic Status on the Brain


- The model:

`$$y_{i} = \mathbf{d}_{\text{s},i}'\boldsymbol{\beta}_{\text{s}} + d_{h,i}\left\{ \mathbf{x}_{\text{h},i}' \boldsymbol{\beta}_{\text{h}} + f\left(a_{i}\right)\right\} +  \eta_{1} \mathbf{z}_{i}' \boldsymbol{\lambda} + d_{\text{h},i} \eta_{2} + \epsilon_{i}$$`

- When `\(i\)` is a hippocampus measurement, `\(\mathbf{z}_{i} = (1,a_{i})'\)` and `\(\boldsymbol{\lambda}\)` contains offset and interaction effect of latent socioeconomic status on hippocampus volume.


---

# Effect of Socioeconomic Status on the Brain

.pull-left[

- We found no evidence of interaction effect, but some evidence of an offset effect.

- Consistent with the hypothesis that socioeconomic status mainly affects early-life brain developement. Or just lack of power.

] 
.pull-right[
![](figures/ses.png)
]



---

class: inverse, middle, center

# Further Work

---

## Further Work

- Study the accuracy of Laplace approximation through importance sampling.

- Let smooth functions depend on latent variables. Leads to `\(\chi^{2}\)` distributed random variables.

- Optimize code and create descent API.

- Utilize the framework to answer research questions in cognitive neuroscience.


Interested in working with similar applications? Postdoc position will soon be announced. Contact me for details.



---

class: inverse, middle, center

# Some Additional Details

---

class: inverse, middle, center

# Transformation from GALAMM to GLLAMM



---

## Semiparametric Estimation

Might have a basis like this.

&lt;img src="presentation_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;


---

## Semiparametric Estimation

Basis can be transformed:

.pull-left[

&lt;img src="presentation_files/figure-html/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

]
.pull-right[

&lt;img src="presentation_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;

]



---

## Semiparametric Estimation

Look closer at the transformed basis:

&lt;img src="presentation_files/figure-html/unnamed-chunk-5-1.svg" style="display: block; margin: auto;" /&gt;


.footnote[Dates back to: Kimeldorf, G. S., &amp; Wahba, G. (1970). A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines. Annals of Mathematical Statistics, 41(2), 495–502. https://doi.org/10.1214/aoms/1177697089
]



---

class: inverse, middle, center

# Implementation Details

---

## Autodiff and Sparse Matrices

- Sparse matrix computations in C++ using Eigen (https://eigen.tuxfamily.org/).

- Rather than `float` or `double`, we fill the Eigen matrices with dual numbers from autodiff (https://autodiff.github.io/), tracing derivatives in the computational graph.

  - First-order dual numbers during optimization with L-BFGS-B to get gradient.
  - Second-order dual numbers at convergence, to get exact Hessian, and thus covariance matrix.
  
--

- Updating `\(\mathbf{X}\)` and `\(\mathbf{Z}\)` at each likelihood evaluation, since they contain parameters. Solved by mapping parameters to values in the compressed sparse column representation of sparse matrices.

- Mapping between R and C++ objects using RcppEigen, and base R's `optim()` function for likelihood maximization.


---

## Autodiff

- Scalar valued function `f()` taking vector arguments


```cpp
#include &lt;autodiff/forward/dual.hpp&gt;
#include &lt;autodiff/forward/dual/eigen.hpp&gt;
using namespace autodiff;

template &lt;typename T&gt;
T f(const Eigen::Array&lt;T, Eigen::Dynamic, 1&gt;&amp; x)
{
    return (x * x.exp()).sum();
}

```


---

## Autodiff

- Scalar type defines what you can get:


```cpp
int main()
{
    ArrayXdual1st x(5); // 1st derivative = gradient
    x &lt;&lt; 1, 2, 3, 4, 5;
    Eigen::VectorXd g = gradient(f&lt;dual1st&gt;, wrt(x), at(x));
    
    ArrayXdual2nd y(5); // 2nd derivative = Hessian
    y &lt;&lt; 1, 2, 3, 4, 5;
    Eigen::MatrixXd H = hessian(f&lt;dual2nd&gt;, wrt(y), at(y));
    
    Eigen::ArrayX&lt;double&gt; z(5); // only function value
    z &lt;&lt; 1, 2, 3, 4, 5;
    double u = f&lt;double&gt;(z);
}
```


- Log-likelihood implementation for GALAMMs contains classes, header files, branching, and loops, but autodiff faithfully gives exact derivatives.



---

class: inverse, middle, center
 
# Thank you
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
